###################################################################
#Shell / Terminal
#Update Python 
sudo yum update
su [password: cloudera]
sudo yum install centos-release-scl
sudo yum install rh-python36
scl enable rh-python36 bash
python ––version
sudo yum groupinstall "Development Tools"
mkdir ~/python_project

###################################################################
#Launch Python Virtual Environment & Jupyter Notebook
cd ~/python_project
scl enable rh-python36 bash
python –m venv python_project_venv
source python_project_venv/bin/activate
jupyter notebook

pip install --upgrade pip
sudo pip3 install pandas
sudo pip3 install numpy
sudo pip3 install pyspark
sudo pip3 install ipython-cypher
sudo pip3 install jupyter


#Remove safe mode of namenode
sudo -u hdfs hdfs dfsadmin -safemode leave


#Add to etc/hadoop/conf/hdfs-site.xml
#<property>
#  <name>dfs.permissions</name>
#  <value>false</value>
#</property>


#Start HDFS and Yarn
start-dfs.sh
start-yarn.sh

#If needed: Stop yarn
stop-dfs.sh

###################################################################
#In Shell: Install Jupyter Notebook
sudo apt-get install -y python-dev
sudo pip install --upgrade pip
sudo pip install jupyter
sudo apt-get install -y python-seaborn python-pandas
sudo apt-get install -y ttf-bitstream-vera
sudo pip3 install jupyter
sudo ipython3 kernelspec install-self

#Configure Jupyter Notebook
jupyter notebook --generate-config
#Navigate to config file (/home/pi/.jupyter/)
#Add the following
c.NotebookApp.allow_origin = '*'
c.NotebookApp.ip = '0.0.0.0'

###################################################################
#Install UFW firewall
sudo apt-get install ufw
sudo ufw status
sudo ufw enable
sudo ufw status verbose
sudo ufw allow ssh
sudo ufw status
sudo ufw allow #port:#port

#Launch Jupyter Notebook
jupyter notebook

###################################################################
#In Shell
#Make directory
hdfs fs -mkdir /tmp

#Upload file
hdfs fs -put /home/pi/Desktop/Recipe_Ingredients_Graph.csv /tmp/

#List directory and check if file exists in directory
hdfs fs -ls /tmp

#View content
hdfs dfs -cat /tmp/Recipe_Ingredients_Graph.csv

###################################################################
#Install neo4j
yum install neo4j
#OR ELSE log in as root (password: cloudera)
rpm --import https://debian.neo4j.org/neotechnology.gpg.key
cat <<EOF>  /etc/yum.repos.d/neo4j.repo
[neo4j]
name=Neo4j RPM Repository
baseurl=https://yum.neo4j.org/stable
enabled=1
gpgcheck=1
EOF

yum install neo4j-3.5.12
rpm -qa | grep neo

#Search for installed package directory
rpm -ql neo4j


#Configure etc/neo4j/neo4j.conf via sudo nano
#dbms.directories.import=import
dbms.security.auth_enabled=false


#Run neo4j in browser localhost:7474
sudo neo4j start

#Location hdfs://quickstart.cloudera:8020
50070


#Launch Jupyter Notebook in Shell
jupyter notebook
localhost:8889


############################################################################
#Install R
sudo yum install R
R

#In R
install.packages("rJava")
install.packages("RJDBC",dep=TRUE)



############################################################################
#Python

##NEW: Read from HDFS
df = spark.read.load("hdfs://quickstart.cloudera:8020/user/cloudera/RI.csv", format="csv")
df.show()

pdf = df.select("*").toPandas()


#Load HDFS files
pip3 install pydoop
import pydoop.hdfs as hdfs
hdfs.load('#dir')

#Load HDFS files
pip3 install hdfs[avro,dataframe,kerberos]
from hdfs import Config
from hdfs.ext.dataframe import read_dataframe, write_dataframe
import pandas as pd



from pyspark import SparkContext, SparkConf
sc = SparkContext()
#Load data
data = sc.textFile("hdfs://localhost:9000/RI.csv")

type(data)

from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local").appName("Ingestion").getOrCreate()


dataframe2 = spark.read.format("csv").option("header", "true").option("mode", "DROPMALFORMED").load("hdfs://localhost:9000/RI.csv")

type(dataframe2)


############################################################################
#Spark
#Find namenode director
hdfs getconf -confKey fs.defaultFS

#Load txt
val input = sc.textFile("hdfs://localhost:9000/hello.txt")

#Load csv
spark.read.option("header","true").csv("hdfs://localhost:9000/RI.csv").show


###################################################################
#MYSQL
mysql -u root

SHOW GLOBAL VARIABLES LIKE 'PORT';

create database food;
use food;

CREATE TABLE IF NOT EXISTS `food`.`RI` ( `id` INT NOT NULL , `Recipe` VARCHAR(255) NOT NULL , `Ingredients` VARCHAR(255) NOT NULL , `Creation` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ) ENGINE = CSV;

LOAD DATA LOCAL INFILE '/home/pi/Desktop/RI.csv'
INTO TABLE RI
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;


###################################################################
#PHP NEO4J INSTALLATION
curl -sS https://getcomposer.org/installer | php
chmod +x composer.phar
mv composer.phar /usr/local/bin/composer
composer -V