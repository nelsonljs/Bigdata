{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting my folder directory\n",
    "# Spacings are fine!\n",
    "\n",
    "from os import chdir\n",
    "path = \"C://Users//nelso//Documents//Data Science//Python//Scripts//pySpark\"\n",
    "chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\devtools\\spark-2.4.4-bin-hadoop2.7\")\n",
    "print(\"Hello! findspark ran fine.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"first app\")\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"SparkContext initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Testing PySpark dataframes\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print('SparkSession Initialized')\n",
    "df = spark.read.load(\"sampledf.csv\",\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "df.select(\"Recipe\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Using inbuilt tokenizers\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Recipe\", outputCol=\"words\")\n",
    "#pipeline = Pipeline(stages=[tokenizer])\n",
    "\n",
    "wordsData = tokenizer.transform(df.select(\"Recipe\"))\n",
    "wordsData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Using user defined functions\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "mywords = ['basil','yeast','buns']\n",
    "\n",
    "def lowercase(x):\n",
    "    lowerW = x.lower()\n",
    "    return lowerW\n",
    "\n",
    "def matching(x):\n",
    "    return bool(set(x) & set(mywords))\n",
    "\n",
    "def find_letters(x):\n",
    "    first_cap = re.search('[a-z]',x).group(0)\n",
    "    return first_cap\n",
    "\n",
    "lowercasing_udf = udf(lowercase, StringType())\n",
    "matching_udf = udf(matching, BooleanType())\n",
    "letters_udf = udf(find_letters, StringType())\n",
    "\n",
    "wordsData2 = df.withColumn('lowercase', lowercasing_udf('Recipe')).select('Recipe','lowercase')\n",
    "wordsData2.show(5)\n",
    "\n",
    "wordsData3 = wordsData.withColumn('match',matching_udf('words')).select('Recipe','words','match') \\\n",
    ".withColumn('letters', letters_udf('Recipe')).select('Recipe','words','match','letters')\n",
    "wordsData3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Using user defined functions - from NLTK package \n",
    "### THIS SECTION DOESNT WORK\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "# word tokenizer\n",
    "# This will not work, because each slave node does not have the NLTK installed!\n",
    "import nltk\n",
    "nltk.data.path.append(\"C://Users//nelso//Documents//Data Science//Python\")\n",
    "def nltk_tok(x):\n",
    "    token = nltk.word_tokenize(x)\n",
    "    return token\n",
    "print(nltk_tok(\"Hello!, Big data.\"))\n",
    "\n",
    "token_udf = udf(nltk_tok, StringType())\n",
    "\n",
    "wordsData4 = df.withColumn('tokens', token_udf('Recipe')).select('Recipe','tokens')\n",
    "wordsData4.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mytest.txt') as file:\n",
    "    myfile = file.read().splitlines()\n",
    "    \n",
    "print(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "\n",
    "mydata = sc.textFile('sampledf.csv')\n",
    "\n",
    "a = mydata \\\n",
    ".map(lambda line: line.split(',')) \\\n",
    ".collect()\n",
    "\n",
    "print(a)\n",
    "\n",
    "#df = mydata.toDF()\n",
    "\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5,6,8,9,10]\n",
    "\n",
    "# sc = pyspark.SparkContext()\n",
    "a = sc.parallelize(data).map(lambda x: x*x).collect()\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
