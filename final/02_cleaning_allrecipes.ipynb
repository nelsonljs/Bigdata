{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory changed to: /home/centos/BigData/02_cleaning\n"
     ]
    }
   ],
   "source": [
    "####----Check correct working directory----####\n",
    "\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "if(wd != '/home/centos/BigData/02_cleaning/'):\n",
    "    from os import chdir\n",
    "    chdir(\"/home/centos/BigData/02_cleaning/\")\n",
    "    print(\"Directory changed to: \" + wd)\n",
    "else:\n",
    "    print(\"Correct current directory: \" + wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 525\n"
     ]
    }
   ],
   "source": [
    "####----Import dictionary----####\n",
    "with open('dict/Overall_Master.txt', encoding = 'ISO-8859-1') as file:\n",
    "    ingredient_set = file.read().splitlines()\n",
    "\n",
    "print(\"List length: \" +str(len(ingredient_set)))\n",
    "#print(ingredient_set)\n",
    "#ingredient_set = ingredient_set[1:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#let's try splitting the list into unigram, bi-gram and tri-gram\n",
    "#find out the longest item in list\n",
    "max_length = 0\n",
    "for item in ingredient_list:\n",
    "    temp = item.split(\" \")\n",
    "    count = len(temp)\n",
    "    if (count > max_length):\n",
    "        max_length = count\n",
    "del item, temp, count\n",
    "print(\"Longest length: \" + str(max_length))\n",
    "\n",
    "#Let's split into 4 groups\n",
    "(ingredient_1, ingredient_2, ingredient_3, ingredient_4) = [[], [], [], []]\n",
    "\n",
    "for item in ingredient_list:\n",
    "    temp = item.split(\" \")\n",
    "    check_length = len(temp)\n",
    "    if (check_length == 1):\n",
    "        ingredient_1.append(item)\n",
    "    if (check_length == 2):\n",
    "        ingredient_2.append(item)\n",
    "#     if (check_length == 3):\n",
    "#         ingredient_3.append(item)\n",
    "#     if (check_length == 4):\n",
    "#         ingredient_4.append(item)\n",
    "del temp, item, check_length\n",
    "\n",
    "ingredient_set = []\n",
    "for ing in ingredient_1:\n",
    "    ingredient_set.append(ing)\n",
    "del ing\n",
    "for ing in ingredient_2:\n",
    "    ingredient_set.append(ing)\n",
    "del ing\n",
    "\n",
    "print(ingredient_set)\n",
    "\n",
    "#We need to shorten the ingredient_2 set\n",
    "\n",
    "with open(\"Ingredient_1.txt\", \"w\") as file:\n",
    "    for ing in ingredient_1:\n",
    "        file.write(ing)\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "with open(\"Ingredient_2.txt\", \"w\") as file:\n",
    "    for ing in ingredient_2:\n",
    "        file.write(ing)\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "with open(\"Overall_New.txt\", \"w\") as file:\n",
    "    for ing in ingredient_set:\n",
    "        file.write(ing)\n",
    "        file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/centos/.local/lib/python3.7/site-packages (2.4.4)\n",
      "Requirement already satisfied: py4j==0.10.7 in /home/centos/.local/lib/python3.7/site-packages (from pyspark) (0.10.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Spark connected\n",
      "Spark session created\n"
     ]
    }
   ],
   "source": [
    "####----Connect to Spark via PySpark----####\n",
    "%pip install pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col, lit, monotonically_increasing_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, CountVectorizer\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "print(\"Spark connected\")\n",
    "\n",
    "#spark.close()\n",
    "#spark.stop()\n",
    "\n",
    "\n",
    "#spark = SparkContext(conf = SparkConf().setAppName(\"PySpark\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"value\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark session created\")\n",
    "\n",
    "##Python Java issues: a drawback in using pyspark instead of Scala\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install itertools\n",
    "# from itertools import compress\n",
    "\n",
    "#%pip install json\n",
    "#import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- cook_time_minutes: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- error: boolean (nullable = true)\n",
      " |-- footnotes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- instructions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- photo_url: string (nullable = true)\n",
      " |-- prep_time_minutes: long (nullable = true)\n",
      " |-- rating_stars: double (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- time_scraped: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- total_time_minutes: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(author='Stephanie', cook_time_minutes=25, description='I just started adding my favorite things to basic cornbread and I came up with something great!', error=False, footnotes=[], ingredients=['1/2 cup unsalted butter, chilled and cubed', '1 cup chopped onion', '1 3/4 cups cornmeal', '1 1/4 cups all-purpose flour', '1/4 cup white sugar', '1 tablespoon baking powder', '1 1/2 teaspoons salt', '1/2 teaspoon baking soda', '1 1/2 cups buttermilk', '3 eggs', '1 1/2 cups shredded pepperjack cheese', '1 1/3 cups frozen corn kernels, thawed and drained', '2 ounces roasted marinated red bell peppers, drained and chopped', '1/2 cup chopped fresh basil'], instructions=['Preheat oven to 400 degrees F (205 degrees C). Butter a 9x9x2 inch baking pan.', 'Melt 1 tablespoon butter in medium nonstick skillet over medium-low heat. Add onion and saute until tender, about 10 minutes. Cool.', 'Mix cornmeal with the flour, baking powder, sugar, salt, and baking soda in large bowl. Add 7 tablespoons butter and rub with fingertips until mixture resembles coarse meal.', 'Whisk buttermilk and eggs in medium bowl to blend. Add buttermilk mixture to dry ingredients and stir until blended. Mix in cheese, corn, red peppers, basil, and onion. Transfer to prepared pan.', 'Bake cornbread until golden and tester inserted comes out clean, about 45 minutes. Cool 20 minutes in pan. Cut cornbread into squares.'], photo_url='http://images.media-allrecipes.com/userphotos/560x315/582853.jpg', prep_time_minutes=55, rating_stars=4.32, review_count=46, time_scraped=1498204021, title='Basil, Roasted Peppers and Monterey Jack Cornbread', total_time_minutes=100, url='http://allrecipes.com/Recipe/6664/')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####----Load json into Spark for data cleaning----####\n",
    "read_path = \"/home/centos/BigData/01_source/allrecipes*.json\"\n",
    "#read_path = \"hdfs://localhost:9000/01_raw/allrecipes*.json\"\n",
    "\n",
    "recipe_j = spark.read.json(read_path)\n",
    "recipe_j.printSchema()\n",
    "\n",
    "recipe_j.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- instructions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- rating_stars: double (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- photo_url: string (nullable = true)\n",
      " |-- total_time_minutes: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+------------+------------+--------------------+------------------+--------------------+\n",
      "|               title|         description|         ingredients|        instructions|rating_stars|review_count|           photo_url|total_time_minutes|                 url|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------+------------+--------------------+------------------+--------------------+\n",
      "|Basil, Roasted Pe...|I just started ad...|[1/2 cup unsalted...|[Preheat oven to ...|        4.32|          46|http://images.med...|               100|http://allrecipes...|\n",
      "|Crispy Cheese Twists|These are great a...|[1/2 cup Parmesan...|[Combine parmesan...|        4.18|          44|http://images.med...|                 0|http://allrecipes...|\n",
      "|   Mom's Yeast Rolls|This is the best ...|[2 cups hot water...|[Melt margarine i...|        3.65|         168|http://images.med...|                 0|http://allrecipes...|\n",
      "|Sweet Potato Bread I|A Southern deligh...|[1 1/2 cups white...|[Combine sugar an...|         4.7|         344|http://images.med...|                 0|http://allrecipes...|\n",
      "|         Orange Buns|You can make thes...|[1/4 cup butter, ...|[Stir butter and ...|        4.71|           6|http://images.med...|               170|http://allrecipes...|\n",
      "+--------------------+--------------------+--------------------+--------------------+------------+------------+--------------------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####----Create Spark DataFrame----####\n",
    "\n",
    "##Create temporary view to transform to dataframes\n",
    "recipe_j.createOrReplaceTempView(\"recipe\")\n",
    "\n",
    "##Create Spark dataframe\n",
    "recipe_df = spark.sql(\"SELECT title, description, ingredients, instructions, rating_stars, review_count, photo_url, total_time_minutes, url FROM recipe LIMIT 1000\")\n",
    "recipe_df.printSchema()\n",
    "\n",
    "recipe_df.show(5)\n",
    "\n",
    "##Generate into sample\n",
    "# small_sample = recipe_sdf.toPandas()\n",
    "# small_sample.to_csv(\"small_sample.csv\")\n",
    "\n",
    "#Noted cook_time_minutes have quite a bit of zeroes. Use rating_stars and review_count instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UDF successful\n"
     ]
    }
   ],
   "source": [
    "####----Extract ingredient----####\n",
    "import re\n",
    "\n",
    "##User Defined Functions\n",
    "def removeParenthesis(li):\n",
    "    output = []\n",
    "    for text in li:\n",
    "        text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "        text = text.strip()\n",
    "        output.append(text)\n",
    "    return output\n",
    "\n",
    "\n",
    "def extractIngredient(li, ingredient_set):\n",
    "    output = []\n",
    "    for item in li:\n",
    "        temp_output = []\n",
    "        for set_tracker in range(len(ingredient_set)):\n",
    "            check = bool(re.search(ingredient_set[set_tracker], item))\n",
    "            if (check == True):    \n",
    "                temp_output.append(ingredient_set[set_tracker])\n",
    "        if (len(temp_output) != 0):\n",
    "##Comment\n",
    "#             temp_counter = 0\n",
    "#             temp_tracker = 0\n",
    "#             for temp in range(len(temp_output)):\n",
    "#                 count_temp = len(temp_output[temp])\n",
    "#                 if (count_temp > temp_counter):\n",
    "#                     temp_tracker = temp\n",
    "#                     temp_counter = count_temp\n",
    "#             output.append(temp_output[temp_tracker])\n",
    "##Comment\n",
    "            output.append(temp_output[[len(i) for i in temp_output].index(max([len(i) for i in temp_output]))])\n",
    "    return output\n",
    "\n",
    "\n",
    "##Testing:\n",
    "#sample = ['1/2 cup unsalted butter, chilled and cubed', '1 cup chopped onion', '1 3/4 cups cornmeal', '1 1/4 cups all-purpose flour', '1/4 cup white sugar', '1 tablespoon baking powder', '1 1/2 teaspoons salt', '1/2 teaspoon baking soda', '1 1/2 cups buttermilk', '3 eggs', '1 1/2 cups shredded pepperjack cheese', '1 1/3 cups frozen corn kernels, thawed and drained', '2 ounces roasted marinated red bell peppers, drained and chopped', '1/2 cup chopped fresh basil']\n",
    "#test = extractIngredient(sample, ingredient_set)\n",
    "#print(test)\n",
    "\n",
    "\n",
    "##Create User Defined Function\n",
    "udf_removeParen = udf(removeParenthesis, ArrayType(StringType()))\n",
    "udf_extractIngredient = udf(lambda x: extractIngredient(x,ingredient_set), ArrayType(StringType()))\n",
    "udf_countList = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "print(\"UDF successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####----Parallel processing----####\n",
    "# %pip install multiprocessing\n",
    "# import multiprocessing as mp\n",
    "\n",
    "# #Define an output queue\n",
    "# output = mp.Queue()\n",
    "\n",
    "\n",
    "# def cleaning():{\n",
    "# ####----Create ingredient table----####\n",
    "# ##Create ingredient table\n",
    "# recipe_j.select([\"title\", \"ingredients\"])\n",
    "# #ingredient_tmp_df = recipe_df.select([\"title\", \"ingredients\"])\n",
    "\n",
    "# ingredient_tmp_df.withColumn('rm_paren', udf_removeParen('ingredients')).select('title','ingredients','rm_paren') \\\n",
    "#                          .withColumn('ingredient_extract', udf_extractIngredient('rm_paren')).select('title','ingredient_extract') \\\n",
    "#                          .withColumn('extract_count', udf_countList('ingredient_extract')).select('title','ingredient_extract', 'extract_count')\n",
    "# return ingredient_df\n",
    "# }\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# ##Setup a list of processes that we want to run\n",
    "# processes = [mp.Process(target = cleaning)]\n",
    "\n",
    "\n",
    "# ingredient_df.show(5)\n",
    "\n",
    "# ##Run processes\n",
    "# for p in processes:\n",
    "#     p.start()\n",
    "\n",
    "# ##Exit the completed processes\n",
    "# for p in processes:\n",
    "#     p.join()\n",
    "\n",
    "# ##Get process results from the output queue\n",
    "# results = [output.get() for p in processes]\n",
    "# results.sort()\n",
    "# results = [r[1] for r in results]\n",
    "# print(results)\n",
    "\n",
    "# pool = mp.Pool(processes=4)\n",
    "# results = [pool.apply(cleaning)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Experimentation of creating shared matrix (TO BE DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+\n",
      "|               title|  ingredient_extract|extract_count|\n",
      "+--------------------+--------------------+-------------+\n",
      "|Basil, Roasted Pe...|[butter, onion, c...|           14|\n",
      "|Crispy Cheese Twists|[cheese, pepper, ...|            5|\n",
      "|   Mom's Yeast Rolls|[water, margarine...|            8|\n",
      "|Sweet Potato Bread I|[sugar, vegetable...|           11|\n",
      "|         Orange Buns|[butter, sugar, m...|           10|\n",
      "+--------------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####----Create ingredient table----####\n",
    "##Create ingredient table\n",
    "ingredient_tmp_df = recipe_j.select([\"title\", \"ingredients\"])\n",
    "#ingredient_tmp_df = recipe_df.select([\"title\", \"ingredients\"])\n",
    "\n",
    "\n",
    "ingredient_df = ingredient_tmp_df.withColumn('rm_paren', udf_removeParen('ingredients')).select('title','ingredients','rm_paren') \\\n",
    "                         .withColumn('ingredient_extract', udf_extractIngredient('rm_paren')).select('title','ingredient_extract') \\\n",
    "                         .withColumn('extract_count', udf_countList('ingredient_extract')).select('title','ingredient_extract', 'extract_count')\n",
    "ingredient_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert into list form for graph\n",
    "ingredient_graph = ingredient_df.withColumn('exploded',explode('ingredient_extract')) \\\n",
    "                      .select(col('title').alias('Recipe'),col('exploded').alias('Ingredient'))\n",
    "#                       .withColumn(\"Frequency\", lit(1))\n",
    "ingredient_graph = ingredient_graph.filter(ingredient_graph.Ingredient != \"\")\n",
    "ingredient_graph.show(20)\n",
    "\n",
    "\n",
    "# graph_sample = ingredient_graph.toPandas()\n",
    "# graph_sample.to_csv(\"graph_sample.csv\")\n",
    "\n",
    "##Let's try convert ingredient into count_frequency (BAD WAY, don't do)\n",
    "# ingredient_freq = ingredient_graph.groupBy(\"Recipe\").pivot(\"Ingredient\").sum(\"Frequency\")\n",
    "# ingredient_freq.show()\n",
    "# test = ingredient_freq.toPandas()\n",
    "# test.to_csv(\"test.csv\")\n",
    "# print(\"test done\")\n",
    "\n",
    "\n",
    "##Proof of concept (Need to do !=)\n",
    "# print(ingredient_graph.filter(ingredient_graph.Ingredient.isNull()).count())\n",
    "# ingredient_graph = ingredient_graph.na.drop(subset=[\"Recipe\"])\n",
    "# test = ingredient_graph.filter(ingredient_graph.Recipe == \"Grandma Emma's Spice Loaf\")\n",
    "# test.show()\n",
    "# test2 = test.filter(ingredient_graph.Ingredient != \"\")\n",
    "# test2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----User Defined Function - Feature Engineering----####\n",
    "##Labels: Vegetarian, Lactose, Nut, Seafood\n",
    "\n",
    "\n",
    "#Safer to start off as non-vegetarian\n",
    "def detectVege(li, vegDetect_list):\n",
    "    label = 0\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in vegDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "\n",
    "#Safer to start off as positive allergy\n",
    "def detectNut(li):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (\"nut\" in text):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "\n",
    "def detectDairy(li):\n",
    "    label = 1\n",
    "    dairy_list = [\"cheese\", \"milk\", \"yoghurt\", \"cream\"]\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (text in dairy_list):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "\n",
    "def detectSeafood(li, seaDetect_list):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in seaDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "\n",
    "##Read dictionary of food in food categories\n",
    "with open(\"dict/meatDetect.txt\", encoding=\"ISO-8859-1\") as meat_file:\n",
    "    vegDetect_list = meat_file.read().splitlines()\n",
    "\n",
    "with open(\"dict/seafoodDetect.txt\", encoding=\"ISO-8859-1\") as seafood_file:\n",
    "    seaDetect_list = seafood_file.read().splitlines()\n",
    "    \n",
    "    \n",
    "##Create User Defined Function to detect and label food categories\n",
    "udf_detectVege = udf(lambda x: detectVege(x, vegDetect_list), IntegerType())\n",
    "udf_detectNut = udf(detectNut, IntegerType())\n",
    "udf_detectDairy = udf(detectDairy, IntegerType())\n",
    "udf_detectSeafood = udf(lambda x: detectSeafood(x, seaDetect_list), IntegerType())\n",
    "\n",
    "\n",
    "##Labelling of ingredients\n",
    "ingredient_label_df = ingredient_df.select(\"title\", \"ingredient_extract\")\n",
    "ingredient_label_df = ingredient_label_df.withColumn(\"vegetarian\", udf_detectVege(\"ingredient_extract\")) \\\n",
    "                           .withColumn(\"nut\", udf_detectNut(\"ingredient_extract\")) \\\n",
    "                           .withColumn(\"lactose\", udf_detectDairy(\"ingredient_extract\")) \\\n",
    "                           .withColumn(\"seafood\", udf_detectSeafood(\"ingredient_extract\"))\n",
    "\n",
    "ingredient_label_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = labelling_df.toPandas()\n",
    "# test.head()\n",
    "#test.to_csv(\"test.csv\")\n",
    "# test = test.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Convert Spark Data Frame to Pandas Data Frame----####\n",
    "#ingredient_tbl_pdf = ingredient_graph.toPandas()\n",
    "#ingredient_tbl_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Save Pandas Data Frame----####\n",
    "\n",
    "# import re\n",
    "# import time\n",
    "# export_filename = 'recipe_allrecipes_' + (time.strftime('%Y%m%d-%H%M%S')) + '.json'\n",
    "# ingredient_tbl_pdf.to_json(export_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Export Spark Data Frame----####\n",
    "\n",
    "import re\n",
    "import time\n",
    "export_filename = 'recipe_allrecipes_' + (time.strftime('%Y%m%d-%H%M%S')) + '.json'\n",
    "ingredient_df.write.save(export_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Upload file into HDFS----####\n",
    "import os \n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "##Create path to HDFS\n",
    "hdfs_path = os.path.join(os.sep, 'user', 'centos', '/02_store/' + export_filename)\n",
    "\n",
    "##Put files into HDFS\n",
    "put_file = Popen([\"hdfs\", \"dfs\", \"-put\", export_filename, hdfs_path], stdin = PIPE, bufsize=-1)\n",
    "put_file.communicate()\n",
    "\n",
    "print('\\nUpload completed and file stored in hdfs://localhost:9000' + hdfs_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Check below: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Addition of other details\n",
    "ingredient_tbl_df = ingredient_df.select(\"title\", \"rm_complete\")\n",
    "ingredient_tbl_df = ingredient_tbl_df.withColumn(\"Ing_Str\", udf_convertString(\"rm_complete\"))\n",
    "# cleanIngredient_table = cleanIngredient_table.withColumn(\"Ing_Str\", udf_convertString2(\"rm_complete\"))\n",
    "ingredient_tbl_df = ingredient_tbl_df.select(\"title\", col('Ing_Str').alias('Ingredient'))\n",
    "ingredient_tbl_df = ingredient_tbl_df.withColumn(\"ID\", monotonically_increasing_id())\n",
    "\n",
    "ingredient_tbl_df.show(5)\n",
    "ingredient_tbl_df.count()\n",
    "\n",
    "ingredient_tbl_pdf = ingredient_tbl_df.toPandas()\n",
    "#ingredient_tbl_pdf.to_csv(\"RI.csv\")\n",
    "\n",
    "# cleanIngredient_table.createOrReplaceTempView('df')\n",
    "# cleanIngredient_table = spark.sql('select row_number() over (order by \"ID\") as num, * from df')\n",
    "# cleanIngredient_table.show(5)\n",
    "# cleanIngredient_table.count()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Ingredient\", outputCol=\"Component\")\n",
    "componentData = tokenizer.transform(cleanIngredient_table)\n",
    "hashingTF = HashingTF(inputCol=\"Component\", outputCol=\"Ing_Component\")\n",
    "featurizedData = hashingTF.transform(componentData)\n",
    "idf = IDF(inputCol=\"Ing_Component\", outputCol=\"Ing_Feature\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "normalizer = Normalizer(inputCol=\"Ing_Feature\", outputCol=\"norm\")\n",
    "similarity = normalizer.transform(rescaledData)\n",
    "sim_matrix = similarity.select(\"ID\", \"title\", \"norm\")\n",
    "sim_matrix.show()\n",
    "sim_test = sim_matrix.rdd.map(lambda row:IndexedRow(row.ID, row.norm.toArray()))\n",
    "type(sim_test)\n",
    "\n",
    "# mat = IndexedRowMatrix(similarity.select(\"title\", \"norm\")\\\n",
    "#                      .rdd.map(lambda row: IndexedRow(row.title, row.norm.toArray()))).toBlockMatrix()\n",
    "# dot = mat.multiply(mat.transpose())\n",
    "# dot.toLocalMatrix().toArray()\n",
    "\n",
    "# dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "# data.alias(\"i\").join(data.alias(\"j\"), col(\"i.title\") < col(\"j.title\"))\\\n",
    "#     .select(\n",
    "#         col(\"i.title\").alias(\"i\"), \n",
    "#         col(\"j.title\").alias(\"j\"), \n",
    "#         dot_udf(\"i.norm\", \"j.norm\").alias(\"dot\"))\\\n",
    "#     .sort(\"i\", \"j\")\\\n",
    "#     .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat = IndexedRowMatrix(sim_test).toBlockMatrix()\n",
    "dot = sim_mat.multiply(sim_mat.transpose())\n",
    "dot.toLocalMatrix().toArray()\n",
    "dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "\n",
    "similarity.alias(\"i\").join(similarity.alias(\"j\"), col(\"i.ID\") < col(\"j.ID\"))\\\n",
    "    .select(\n",
    "        col(\"i.ID\").alias(\"i\"), \n",
    "        col(\"j.ID\").alias(\"j\"), \n",
    "        dot_udf(\"i.ID\", \"j.ID\").alias(\"cosine\"))\\\n",
    "    .sort(\"i\", \"j\")\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cv = CountVectorizer(inputCol=\"Ing_Str\", outputCol=\"Ing_Feature\", vocabSize=3, minDF=2.0)\n",
    "model = cv.fit(cleanIngredient_table)\n",
    "result = model.transform(cleanIngredient_table)\n",
    "result.show(5)\n",
    "test = result.toPandas()\n",
    "test.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Let's try create Graphs\n",
    "#Assuming user only wants recipe greater than 4 stars\n",
    "# user_request = allrecipes_df.filter(allrecipes_df.rating_stars > 4.0)\n",
    "user_request.show(5)\n",
    "#let's cache the user_request data\n",
    "# user_request.cache()\n",
    "# user_request.storageLevel\n",
    "# user_request.unpersist()\n",
    "# user_request.storageLevel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#let's try create a graph with the user_request data\n",
    "user_ingredient = user_request.select([\"title\", \"ingredients\"])\n",
    "user_cleanIngredient = user_ingredient.withColumn('rm_paren', udf_removeParen('ingredients')).select('title','ingredients','rm_paren') \\\n",
    "                         .withColumn('rm_after', udf_removeAfter('rm_paren')).select('title','ingredients','rm_paren','rm_after') \\\n",
    "                         .withColumn('rm_numbers', udf_removeNumbers('rm_after')).select('title','ingredients','rm_paren','rm_after','rm_numbers') \\\n",
    "                         .withColumn('rm_complete', udf_removeStop('rm_numbers')).select('title','ingredients','rm_paren','rm_after','rm_numbers','rm_complete')\n",
    "user_cleanIngredient.show(5)\n",
    "\n",
    "#to convert into graph form\n",
    "user_vertices = user_request.select([\"title\", \"rating_stars\", \"review_count\", \"total_time_minutes\"])\n",
    "\n",
    "user_edges = user_cleanIngredient.withColumn('exploded',explode('rm_complete')) \\\n",
    "                      .select(col('title').alias('Recipes'),col('exploded').alias('Ingredients')) \\\n",
    "                      .withColumn(\"Relationship\", lit(\"Contains\"))\n",
    "\n",
    "user_greater4 = user_edges.toPandas()\n",
    "user_greater4.to_csv(\"user_filtered.csv\")\n",
    "\n",
    "user_vertices.show(5)\n",
    "user_edges.show(5)\n",
    "\n",
    "user_graph = GraphFrame(user_vertices, user_edges)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Aggregated User-defined functions\n",
    "def removePara(text):\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "    return text\n",
    "def removeNum(text):\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    return text\n",
    "def removePunc(text):\n",
    "    text = re.sub(r\"[//]\", \" \", text)\n",
    "    return text\n",
    "def removeComma(text):\n",
    "    text = re.sub(r\",[^,]*$\", \"\", text)\n",
    "    return text\n",
    "def removeStop(text, stoplist):\n",
    "    text_tok = text.split(\" \")\n",
    "    text_output = [tok for tok in text_tok if tok not in stoplist]\n",
    "    text_output = \" \".join(text_output)\n",
    "    return text_output\n",
    "\n",
    "def removeRules(li, stoplist):\n",
    "    output = []\n",
    "    for text in li:\n",
    "        text = removePara(text)\n",
    "        text = removeNum(text)\n",
    "        text = removePunc(text)\n",
    "        text = removeComma(text)\n",
    "        text = text.strip()\n",
    "        text = removeStop(text, stoplist)\n",
    "        output.append(text)\n",
    "    return output\n",
    "\n",
    "def removeRu2(li, stoplist as list):\n",
    "    output = []\n",
    "    for text in li:\n",
    "        text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = re.sub(r\"[//]\", \" \", text)\n",
    "        text = re.sub(r\",[^,]*$\", \"\", text)\n",
    "        text = text.strip()\n",
    "        text_tok = text.split(\" \")\n",
    "        text_output = [tok for tok in text_tok if tok not in stoplist]\n",
    "        text_output = \" \".join(text_output)\n",
    "        output.append(text_output)\n",
    "    return output\n",
    "\n",
    "sample = [\"1/2 cup unsalted butter, chilled and cubed\", \"1 cup chopped onion\", \"1 3/4 cups cornmeal\", \"1 1/4 cups all-purpose flour\", \"1/4 cup white sugar\", \"1 tablespoon baking powder\", \"1 1/2 teaspoons salt\", \"1/2 teaspoon baking soda\", \"1 1/2 cups buttermilk\", \"3 eggs\", \"1 1/2 cups shredded pepperjack cheese\", \"1 1/3 cups frozen corn kernels, thawed and drained\", \"2 ounces roasted marinated red bell peppers, drained and chopped\", \"1/2 cup chopped fresh basil\"]\n",
    "sample = removeRu2(sample, stopwords)\n",
    "print(sample)\n",
    "\n",
    "#create uder defined function\n",
    "udf_removeRules = udf(removeRules, ArrayType(StringType()))\n",
    "udf_removeRu2 = udf(removeRu2, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Create new column and test function\n",
    "#Spark Dataframes are immutable. Thus we have to create new columns\n",
    "test = ingredients_df.withColumn(\"New\", udf_removeRu2(ingredients_df[\"ingredients\"], stopwords))\n",
    "test.show(5)\n",
    "\n",
    "# test_pd = test.toPandas()\n",
    "# test_pd.head()\n",
    "# test_pd.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
