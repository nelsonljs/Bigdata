{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Check correct working directory----####\n",
    "\n",
    "import os\n",
    "wd = os.getcwd()\n",
    "if(wd != '/home/centos/BigData/02_cleaning/'):\n",
    "    from os import chdir\n",
    "    chdir(\"/home/centos/BigData/02_cleaning/\")\n",
    "    print(\"Directory changed to: \" + wd)\n",
    "else:\n",
    "    print(\"Correct current directory: \" + wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Import dictionary----####\n",
    "with open('dict/Overall_Master.txt', encoding = 'ISO-8859-1') as file:\n",
    "    ingredient_set = file.read().splitlines()\n",
    "\n",
    "print(\"List length: \" +str(len(ingredient_set)))\n",
    "#print(ingredient_set)\n",
    "#ingredient_set = ingredient_set[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Connect to Spark via PySpark----####\n",
    "%pip install pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf, explode, col, lit, monotonically_increasing_id, unix_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, CountVectorizer\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import findspark\n",
    "print(\"Connecting Spark\")\n",
    "\n",
    "#spark.close()\n",
    "sc.stop()\n",
    "\n",
    "\n",
    "#Settings necessary for sandboxing otherwise Pyspark too slow on standalone\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "sc = SparkContext(\"local\", \"App Name\")\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "sqlContext.setConf(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"12000\")\n",
    "#improve toPandas() \n",
    "sqlContext.setConf(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "print(\"Spark Context Created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark SQL basic example\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "# print(\"Spark Session Created\")\n",
    "#Python Java issues: a drawback in using pyspark instead of Scala"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of filtering words\n",
    "with open(\"dict/Overall_Master.txt\") as file:\n",
    "    ingredient_set = file.read().splitlines()\n",
    "print(\"List length: \" + str(len(ingredient_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Load json into Spark for data cleaning----####\n",
    "read_path = \"/home/centos/BigData/01_source/bbc*.json\"\n",
    "#read_path = \"hdfs://localhost:9000/01_raw/allrecipes*.json\"\n",
    "\n",
    "recipe_j = sqlContext.read.json(read_path)\n",
    "recipe_j.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####----Create Spark DataFrame----####\n",
    "\n",
    "##Create temporary view to transform to dataframes\n",
    "recipe_j.createOrReplaceTempView(\"recipe\")\n",
    "\n",
    "##Create Spark dataframe\n",
    "recipe_df = sqlContext.sql(\"SELECT title, description, ingredients, instructions, rating_stars, review_count, photo_url, total_time_minutes, url FROM recipe LIMIT 100\")\n",
    "recipe_df.printSchema()\n",
    "\n",
    "recipe_df.show(5)\n",
    "\n",
    "##Generate into sample\n",
    "# small_sample = recipe_sdf.toPandas()\n",
    "# small_sample.to_csv(\"small_sample.csv\")\n",
    "\n",
    "#Noted cook_time_minutes have quite a bit of zeroes. Use rating_stars and review_count instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Extract ingredient----####\n",
    "import re\n",
    "\n",
    "##User-defined Functions\n",
    "def removeParenthesis(li):\n",
    "    output = []\n",
    "    for text in li:\n",
    "        text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "        text = text.strip()\n",
    "        output.append(text)\n",
    "    return output\n",
    "\n",
    "def extractIngredient(li, ingredient_set):\n",
    "    output = []\n",
    "    for item in li:\n",
    "        temp_output = []\n",
    "        for set_tracker in range(len(ingredient_set)):\n",
    "            check = bool(re.search(ingredient_set[set_tracker], item))\n",
    "            if (check == True):    \n",
    "                temp_output.append(ingredient_set[set_tracker])\n",
    "        if (len(temp_output) != 0):\n",
    "            temp_counter = 0\n",
    "            temp_tracker = 0\n",
    "            for temp in range(len(temp_output)):\n",
    "                count_temp = len(temp_output[temp])\n",
    "                if (count_temp > temp_counter):\n",
    "                    temp_tracker = temp\n",
    "                    temp_counter = count_temp\n",
    "            output.append(temp_output[temp_tracker])\n",
    "#             output.append(temp_output[[len(i) for i in temp_output].index(max([len(i) for i in temp_output]))])\n",
    "    return output\n",
    "\n",
    "sample = ['1/2 cup unsalted butter, chilled and cubed', '1 cup chopped onion', '1 3/4 cups cornmeal', '1 1/4 cups all-purpose flour', '1/4 cup white sugar', '1 tablespoon baking powder', '1 1/2 teaspoons salt', '1/2 teaspoon baking soda', '1 1/2 cups buttermilk', '3 eggs', '1 1/2 cups shredded pepperjack cheese', '1 1/3 cups frozen corn kernels, thawed and drained', '2 ounces roasted marinated red bell peppers, drained and chopped', '1/2 cup chopped fresh basil']\n",
    "\n",
    "test = extractIngredient(sample, ingredient_set)\n",
    "print(test)\n",
    "print(len(ingredient_set))\n",
    "\n",
    "##Create user defined function\n",
    "udf_removeParen = udf(removeParenthesis, ArrayType(StringType()))\n",
    "udf_extractIngredient = udf(lambda x: extractIngredient(x,ingredient_set), ArrayType(StringType()))\n",
    "udf_countList = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "print(\"UDF Successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Defined Function - Feature Engineering \n",
    "#Labels: Vegetarian, Lactose, Nut, Seafood\n",
    "\n",
    "#Safer to start off as non-vegetarian\n",
    "def detectVege(li, vegDetect_list):\n",
    "    label = 0\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in vegDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "#Safer to start off as positive allergy\n",
    "def detectNut(li):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (\"nut\" in text):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "def detectDairy(li):\n",
    "    label = 1\n",
    "    dairy_list = [\"cheese\", \"milk\", \"yoghurt\", \"cream\"]\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (text in dairy_list):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "def detectSeafood(li, seaDetect_list):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in seaDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "\n",
    "with open(\"dict/vegDetect.txt\") as veg_file:\n",
    "    vegDetect_list = veg_file.read().splitlines()\n",
    "\n",
    "with open(\"dict/seafoodDetect.txt\") as seafood_file:\n",
    "    seaDetect_list = seafood_file.read().splitlines()\n",
    "    \n",
    "#create user defined function\n",
    "udf_detectVege = udf(lambda x: detectVege(x, vegDetect_list), IntegerType())\n",
    "udf_detectNut = udf(detectNut, IntegerType())\n",
    "udf_detectDairy = udf(detectDairy, IntegerType())\n",
    "udf_detectSeafood = udf(lambda x: detectSeafood(x, seaDetect_list), IntegerType())\n",
    "\n",
    "#to stabilise schema with empty column\n",
    "# udf_empty = udf(lambda x: None, StringType())\n",
    "\n",
    "#create variable for time stamp\n",
    "timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Create ingredient table----####\n",
    "##Create ingredient table\n",
    "extracted_df = allbbc_table.withColumn('rm_paren', udf_removeParen('ingredients')) \\\n",
    "                           .withColumn('ingredient_extract', udf_extractIngredient('rm_paren'))\n",
    "\n",
    "extracted_df.cache()\n",
    "extracted_df.show(5)\n",
    "extracted_df.storageLevel\n",
    "# extracted_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create recipe ingredient graph to store in HDFS\n",
    "ingredient_extract = extracted_df.select(\"title\", \"ingredient_extract\")\n",
    "\n",
    "ingredient_graph = ingredient_extract.withColumn('exploded',explode('ingredient_extract')) \\\n",
    "                                   .select(col('title').alias('Recipe'),col('exploded').alias('Ingredient'))\\\n",
    "\n",
    "ingredient_graph = ingredient_graph.filter(ingredient_graph.Ingredient != \"\")\n",
    "#                       .withColumn(\"Frequency\", lit(1))\n",
    "\n",
    "ingredient_graph.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----User Defined Function - Feature Engineering----####\n",
    "##Labels: Vegetarian, Lactose, Nut, Seafood\n",
    "\n",
    "labelled_df = extracted_df.withColumn(\"vegetarian_label\", udf_detectVege(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"nut_label\", udf_detectNut(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"lactose\", udf_detectDairy(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"seafood\", udf_detectSeafood(\"ingredient_extract\")) \\\n",
    "                          .withColumn('extract_count', udf_countList('ingredient_extract'))\\\n",
    "\n",
    "labelled_df = labelled_df.withColumn('timestamp', unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n",
    "\n",
    "# labelled_df = labelled_df.withColumn(\"rating_stars\", udf_empty(\"ingredient_extract\")) \\\n",
    "#                          .withColumn(\"review_count\", udf_empty(\"ingredient_extract\"))\n",
    "\n",
    "\n",
    "labelled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['rm_paren', 'ingredient_extract']\n",
    "labelled_df = labelled_df.drop(*columns_to_drop)\n",
    "labelled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####----Export Spark Data Frame----####\n",
    "\n",
    "import re\n",
    "import time\n",
    "export_filename = 'recipe_bbc_' + (time.strftime('%Y%m%d-%H%M%S')) + '.csv'\n",
    "\n",
    "\n",
    "#let's make our toPandas() faster\n",
    "labelled_pandas = labelled_df.toPandas()\n",
    "labelled_pandas.to_csv(export_filename)\n",
    "# graph_pandas = ingredient_graph.toPandas()\n",
    "# graph_pandas.to_csv(\"ingredient_graph.csv\")\n",
    "\n",
    "#write json\n",
    "# labelled_json = labelled_df.toJSON()\n",
    "# json_output = labelled_json.collect()\n",
    "\n",
    "# with open(\"label_table.json\", \"w\") as file:\n",
    "#     for j in json_output:\n",
    "#         json.dump(j, file)\n",
    "\n",
    "\n",
    "####----Upload file into HDFS----####\n",
    "import os \n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "##Create path to HDFS\n",
    "hdfs_path = os.path.join(os.sep, 'user', 'centos', '/02_store/' + export_filename)\n",
    "\n",
    "##Put files into HDFS\n",
    "put_file = Popen([\"hdfs\", \"dfs\", \"-put -f\", export_filename, hdfs_path], stdin = PIPE, bufsize=-1)\n",
    "put_file.communicate()\n",
    "\n",
    "print('\\nUpload completed and file stored in hdfs://localhost:9000' + hdfs_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(json_output[2])\n",
    "with open(\"allbbc_label.json\", \"w\") as file:\n",
    "    json.dump(j, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create recipe-recipe graph\n",
    "recipeGraph_json = ingredient_extract.toJSON()\n",
    "recipeGraph_json = recipeGraph_json.collect()\n",
    "\n",
    "recipeGraph_list = []\n",
    "for js in recipeGraph_json:\n",
    "    js_output = json.loads(js)\n",
    "    recipeGraph_list.append(js_output)\n",
    "\n",
    "recipeA = []\n",
    "recipeB = []\n",
    "commonIng = []\n",
    "for first in range(len(recipeGraph_list)):\n",
    "    counter = 1\n",
    "    recipe_a = recipeGraph_list[first][\"title\"]\n",
    "    ing_a = recipeGraph_list[first][\"ingredient_extract\"]\n",
    "    for second in range(counter, len(recipeGraph_list)): \n",
    "        recipe_b = recipeGraph_list[second][\"title\"]\n",
    "        ing_b = recipeGraph_list[second][\"ingredient_extract\"]\n",
    "        common = len(set(ing_a).intersection(ing_b))\n",
    "        recipeA.append(recipe_a)\n",
    "        recipeB.append(recipe_b)\n",
    "        commonIng.append(common)\n",
    "    counter = counter + 1\n",
    "\n",
    "recipeGraph_dict = {\"recipeA\":recipeA, \"recipeB\":recipeB, \"common_ing\":commonIng}\n",
    "sharedRecipe_pandas = pd.DataFrame(recipeGraph_dict)\n",
    "export_filename = 'recipe_bbc_Ing' + (time.strftime('%Y%m%d-%H%M%S')) + '.csv'\n",
    "\n",
    "sharedRecipe_pandas.to_csv(export_filename)\n",
    "\n",
    "####----Upload file into HDFS----####\n",
    "import os \n",
    "from subprocess import PIPE, Popen\n",
    "\n",
    "##Create path to HDFS\n",
    "hdfs_path = os.path.join(os.sep, 'user', 'centos', '/02_store/' + export_filename)\n",
    "\n",
    "##Put files into HDFS\n",
    "put_file = Popen([\"hdfs\", \"dfs\", \"-put -f\", export_filename, hdfs_path], stdin = PIPE, bufsize=-1)\n",
    "put_file.communicate()\n",
    "\n",
    "print('\\nUpload completed and file stored in hdfs://localhost:9000' + hdfs_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Similarity matrix (Too expensive)\n",
    "# tokenizer = Tokenizer(inputCol=\"Ingredient\", outputCol=\"Component\")\n",
    "# componentData = tokenizer.transform(cleanIngredient_table)\n",
    "# hashingTF = HashingTF(inputCol=\"Component\", outputCol=\"Ing_Component\")\n",
    "# featurizedData = hashingTF.transform(componentData)\n",
    "# idf = IDF(inputCol=\"Ing_Component\", outputCol=\"Ing_Feature\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# normalizer = Normalizer(inputCol=\"Ing_Feature\", outputCol=\"norm\")\n",
    "# similarity = normalizer.transform(rescaledData)\n",
    "# sim_matrix = similarity.select(\"ID\", \"title\", \"norm\")\n",
    "# sim_matrix.show()\n",
    "# sim_test = sim_matrix.rdd.map(lambda row:IndexedRow(row.ID, row.norm.toArray()))\n",
    "# type(sim_test)\n",
    "\n",
    "# mat = IndexedRowMatrix(similarity.select(\"title\", \"norm\")\\\n",
    "#                      .rdd.map(lambda row: IndexedRow(row.title, row.norm.toArray()))).toBlockMatrix()\n",
    "# dot = mat.multiply(mat.transpose())\n",
    "# dot.toLocalMatrix().toArray()\n",
    "\n",
    "# dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "# data.alias(\"i\").join(data.alias(\"j\"), col(\"i.title\") < col(\"j.title\"))\\\n",
    "#     .select(\n",
    "#         col(\"i.title\").alias(\"i\"), \n",
    "#         col(\"j.title\").alias(\"j\"), \n",
    "#         dot_udf(\"i.norm\", \"j.norm\").alias(\"dot\"))\\\n",
    "#     .sort(\"i\", \"j\")\\\n",
    "#     .show()\n",
    "\n",
    "# sim_mat = IndexedRowMatrix(sim_test).toBlockMatrix()\n",
    "# dot = sim_mat.multiply(sim_mat.transpose())\n",
    "# dot.toLocalMatrix().toArray()\n",
    "# dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "\n",
    "# similarity.alias(\"i\").join(similarity.alias(\"j\"), col(\"i.ID\") < col(\"j.ID\"))\\\n",
    "#     .select(\n",
    "#         col(\"i.ID\").alias(\"i\"), \n",
    "#         col(\"j.ID\").alias(\"j\"), \n",
    "#         dot_udf(\"i.ID\", \"j.ID\").alias(\"cosine\"))\\\n",
    "#     .sort(\"i\", \"j\")\\\n",
    "#     .show()\n",
    "\n",
    "# cv = CountVectorizer(inputCol=\"Ing_Str\", outputCol=\"Ing_Feature\", vocabSize=3, minDF=2.0)\n",
    "# model = cv.fit(cleanIngredient_table)\n",
    "# result = model.transform(cleanIngredient_table)\n",
    "# result.show(5)\n",
    "# test = result.toPandas()\n",
    "# test.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
