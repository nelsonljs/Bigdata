{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark ran\n"
     ]
    }
   ],
   "source": [
    "from os import chdir\n",
    "from itertools import compress\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf, explode, col, lit, monotonically_increasing_id, unix_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, CountVectorizer\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import findspark\n",
    "findspark.init()\n",
    "print(\"spark ran\")\n",
    "\n",
    "chdir(\"C:\\\\Users\\\\chest\\\\Desktop\\\\MTech\\\\Big Data\\\\Project\\\\Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Context Created\n"
     ]
    }
   ],
   "source": [
    "#Settings necessary for sandboxing\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "sc = SparkContext(\"local\", \"App Name\")\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"100\")\n",
    "sqlContext.setConf(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"12000\")\n",
    "#improve toPandas() \n",
    "sqlContext.setConf(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "print(\"Spark Context Created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Created\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark SQL basic example\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "# print(\"Spark Session Created\")\n",
    "# #Python Java issues: a drawback in using pyspark instead of Scala"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List length: 524\n"
     ]
    }
   ],
   "source": [
    "#Creation of filtering words\n",
    "with open(\"Overall_Master.txt\") as file:\n",
    "    ingredient_set = file.read().splitlines()\n",
    "print(\"List length: \" + str(len(ingredient_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- cook_time_minutes: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- error: boolean (nullable = true)\n",
      " |-- footnotes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- instructions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- photo_url: string (nullable = true)\n",
      " |-- prep_time_minutes: long (nullable = true)\n",
      " |-- rating_stars: double (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- time_scraped: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- total_time_minutes: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\chest\\\\Desktop\\\\MTech\\\\Big Data\\\\Project\\\\Final\\\\allrecipes-recipes.json\"\n",
    "allrecipes_df = sqlContext.read.json(path)\n",
    "allrecipes_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+\n",
      "|               title|         ingredients|         description|        instructions|           photo_url|                 url|rating_stars|review_count|total_time_minutes|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|\n",
      "|Lemon Yogurt Poun...|[2 1/4 cups sifte...|This tart and tan...|[Preheat oven to ...|http://images.med...|http://allrecipes...|        4.47|          37|                 0|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|\n",
      "|Pimento Cheese Sp...|[1 pound shredded...|The feta cheese g...|[Stir together th...|http://images.med...|http://allrecipes...|        3.67|           2|                15|\n",
      "|Easiest Homestyle...|[cooking spray, 1...|My family always ...|[Preheat oven to ...|http://images.med...|http://allrecipes...|        4.08|          23|                85|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "allrecipes_df.createOrReplaceTempView(\"allrecipes\")\n",
    "\n",
    "#overall table\n",
    "allrecipes_table = sqlContext.sql(\"SELECT title, ingredients, description, instructions, photo_url, url, rating_stars, review_count, total_time_minutes  FROM allrecipes ORDER BY RAND() LIMIT 1500\")\n",
    "allrecipes_table.show(5)\n",
    "\n",
    "# allrecipes_pandas = allrecipes_table.toPandas()\n",
    "# allrecipes_pandas.to_csv(\"allrecipes_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['butter', 'onion', 'corn', 'flour', 'sugar', '', 'salt', '', 'buttermilk', 'egg', 'cheese', 'corn', 'bell pepper', 'basil']\n",
      "524\n",
      "UDF Successful.\n"
     ]
    }
   ],
   "source": [
    "#User-defined Functions\n",
    "def removeParenthesis(li):\n",
    "    output = []\n",
    "    for text in li:\n",
    "        text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "        text = text.strip()\n",
    "        output.append(text)\n",
    "    return output\n",
    "\n",
    "def extractIngredient(li, ingredient_set):\n",
    "    output = []\n",
    "    for item in li:\n",
    "        temp_output = []\n",
    "        for set_tracker in range(len(ingredient_set)):\n",
    "            check = bool(re.search(ingredient_set[set_tracker], item))\n",
    "            if (check == True):    \n",
    "                temp_output.append(ingredient_set[set_tracker])\n",
    "        if (len(temp_output) != 0):\n",
    "            temp_counter = 0\n",
    "            temp_tracker = 0\n",
    "            for temp in range(len(temp_output)):\n",
    "                count_temp = len(temp_output[temp])\n",
    "                if (count_temp > temp_counter):\n",
    "                    temp_tracker = temp\n",
    "                    temp_counter = count_temp\n",
    "            output.append(temp_output[temp_tracker])\n",
    "#             output.append(temp_output[[len(i) for i in temp_output].index(max([len(i) for i in temp_output]))])\n",
    "    return output\n",
    "\n",
    "sample = ['1/2 cup unsalted butter, chilled and cubed', '1 cup chopped onion', '1 3/4 cups cornmeal', '1 1/4 cups all-purpose flour', '1/4 cup white sugar', '1 tablespoon baking powder', '1 1/2 teaspoons salt', '1/2 teaspoon baking soda', '1 1/2 cups buttermilk', '3 eggs', '1 1/2 cups shredded pepperjack cheese', '1 1/3 cups frozen corn kernels, thawed and drained', '2 ounces roasted marinated red bell peppers, drained and chopped', '1/2 cup chopped fresh basil']\n",
    "\n",
    "test = extractIngredient(sample, ingredient_set)\n",
    "print(test)\n",
    "print(len(ingredient_set))\n",
    "\n",
    "#create user defined function\n",
    "udf_removeParen = udf(removeParenthesis, ArrayType(StringType()))\n",
    "udf_extractIngredient = udf(lambda x: extractIngredient(x,ingredient_set), ArrayType(StringType()))\n",
    "udf_countList = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "print(\"UDF Successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Defined Function - Feature Engineering \n",
    "#Labels: Vegetarian, Lactose, Nut, Seafood\n",
    "\n",
    "#Safer to start off as non-vegetarian\n",
    "def detectVege(li, vegDetect_list):\n",
    "    label = 0\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in vegDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "#Safer to start off as positive allergy\n",
    "def detectNut(li):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (\"nut\" in text):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "def detectDairy(li):\n",
    "    label = 1\n",
    "    dairy_list = [\"cheese\", \"milk\", \"yoghurt\", \"cream\"]\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (text in dairy_list):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "def detectSeafood(li, seaDetect_list):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in seaDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "\n",
    "with open(\"vegDetect.txt\") as veg_file:\n",
    "    vegDetect_list = veg_file.read().splitlines()\n",
    "\n",
    "with open(\"seafoodDetect.txt\") as seafood_file:\n",
    "    seaDetect_list = seafood_file.read().splitlines()\n",
    "    \n",
    "#create user defined function\n",
    "udf_detectVege = udf(lambda x: detectVege(x, vegDetect_list), IntegerType())\n",
    "udf_detectNut = udf(detectNut, IntegerType())\n",
    "udf_detectDairy = udf(detectDairy, IntegerType())\n",
    "udf_detectSeafood = udf(lambda x: detectSeafood(x, seaDetect_list), IntegerType())\n",
    "\n",
    "#to stabilise schema with empty column\n",
    "# udf_empty = udf(lambda x: None, StringType())\n",
    "\n",
    "#create variable for time stamp\n",
    "timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+--------------------+--------------------+\n",
      "|               title|         ingredients|         description|        instructions|           photo_url|                 url|rating_stars|review_count|total_time_minutes|            rm_paren|  ingredient_extract|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+--------------------+--------------------+\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|[1  pre-baked piz...|[, mozzarella, , ...|\n",
      "|Lemon Yogurt Poun...|[2 1/4 cups sifte...|This tart and tan...|[Preheat oven to ...|http://images.med...|http://allrecipes...|        4.47|          37|                 0|[2 1/4 cups sifte...|[flour, sugar, sa...|\n",
      "|Johnsonville® Thr...|[1 (12 inch) pre-...|Squares of cheesy...|[Top pizza crust ...|http://images.med...|http://allrecipes...|         0.0|           0|                30|[1  pre-baked piz...|[, mozzarella, , ...|\n",
      "|Pimento Cheese Sp...|[1 pound shredded...|The feta cheese g...|[Stir together th...|http://images.med...|http://allrecipes...|        3.67|           2|                15|[1 pound shredded...|[cheese, , cheese...|\n",
      "|Easiest Homestyle...|[cooking spray, 1...|My family always ...|[Preheat oven to ...|http://images.med...|http://allrecipes...|        4.08|          23|                85|[cooking spray, 1...|[, salt, vegetabl...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, True, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create ingredient table\n",
    "extracted_df = allrecipes_table.withColumn('rm_paren', udf_removeParen('ingredients')) \\\n",
    "                               .withColumn('ingredient_extract', udf_extractIngredient('rm_paren'))\n",
    "extracted_df.cache()\n",
    "extracted_df.show(5)\n",
    "extracted_df.storageLevel\n",
    "# extracted_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|              Recipe|Ingredient|\n",
      "+--------------------+----------+\n",
      "|Johnsonville® Thr...|mozzarella|\n",
      "|Johnsonville® Thr...|      sage|\n",
      "|Johnsonville® Thr...| pepperoni|\n",
      "|Lemon Yogurt Poun...|     flour|\n",
      "|Lemon Yogurt Poun...|     sugar|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#this section creates recipe ingredient graph to store in HDFS\n",
    "ingredient_extract = extracted_df.select(\"title\", \"ingredient_extract\")\n",
    "\n",
    "ingredient_graph = ingredient_extract.withColumn('exploded',explode('ingredient_extract')) \\\n",
    "                                   .select(col('title').alias('Recipe'),col('exploded').alias('Ingredient'))\\\n",
    "\n",
    "ingredient_graph = ingredient_graph.filter(ingredient_graph.Ingredient != \"\")\n",
    "#                       .withColumn(\"Frequency\", lit(1))\n",
    "\n",
    "ingredient_graph.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create master dataframe\n",
    "labelled_df = extracted_df.withColumn(\"vegetarian_label\", udf_detectVege(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"nut_label\", udf_detectNut(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"lactose\", udf_detectDairy(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"seafood\", udf_detectSeafood(\"ingredient_extract\")) \\\n",
    "                          .withColumn('extract_count', udf_countList('ingredient_extract'))\\\n",
    "\n",
    "labelled_df = labelled_df.withColumn('timestamp', unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- ingredients: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- instructions: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- photo_url: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- rating_stars: double (nullable = true)\n",
      " |-- review_count: long (nullable = true)\n",
      " |-- total_time_minutes: long (nullable = true)\n",
      " |-- vegetarian_label: integer (nullable = true)\n",
      " |-- nut_label: integer (nullable = true)\n",
      " |-- lactose: integer (nullable = true)\n",
      " |-- seafood: integer (nullable = true)\n",
      " |-- extract_count: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['rm_paren', 'ingredient_extract']\n",
    "labelled_df = labelled_df.drop(*columns_to_drop)\n",
    "labelled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output\n",
    "#let's make our toPandas() faster\n",
    "labelled_pandas = labelled_df.toPandas()\n",
    "labelled_pandas.to_csv(\"allrecipes_label.csv\")\n",
    "# graph_pandas = ingredient_graph.toPandas()\n",
    "# graph_pandas.to_csv(\"ingredient_graph.csv\")\n",
    "\n",
    "#write json\n",
    "# labelled_json = labelled_df.toJSON()\n",
    "# json_output = labelled_json.collect()\n",
    "\n",
    "# with open(\"label_table.json\", \"w\") as file:\n",
    "#     for j in json_output:\n",
    "#         json.dump(j, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create recipe-recipe graph\n",
    "recipeGraph_json = ingredient_extract.toJSON()\n",
    "recipeGraph_json = recipeGraph_json.collect()\n",
    "\n",
    "recipeGraph_list = []\n",
    "for js in recipeGraph_json:\n",
    "    js_output = json.loads(js)\n",
    "    recipeGraph_list.append(js_output)\n",
    "\n",
    "recipeA = []\n",
    "recipeB = []\n",
    "commonIng = []\n",
    "for first in range(len(recipeGraph_list)):\n",
    "    counter = 1\n",
    "    recipe_a = recipeGraph_list[first][\"title\"]\n",
    "    ing_a = recipeGraph_list[first][\"ingredient_extract\"]\n",
    "    for second in range(counter, len(recipeGraph_list)): \n",
    "        recipe_b = recipeGraph_list[second][\"title\"]\n",
    "        ing_b = recipeGraph_list[second][\"ingredient_extract\"]\n",
    "        common = len(set(ing_a).intersection(ing_b))\n",
    "        recipeA.append(recipe_a)\n",
    "        recipeB.append(recipe_b)\n",
    "        commonIng.append(common)\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "recipeGraph_dict = {\"recipeA\":recipeA, \"recipeB\":recipeB, \"common_ing\":commonIng}\n",
    "sharedRecipe_pandas = pd.DataFrame(recipeGraph_dict)\n",
    "sharedRecipe_pandas.to_csv(\"allrecipes_sharedGraph.csv\")  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Similarity matrix (Too expensive)\n",
    "# tokenizer = Tokenizer(inputCol=\"Ingredient\", outputCol=\"Component\")\n",
    "# componentData = tokenizer.transform(cleanIngredient_table)\n",
    "# hashingTF = HashingTF(inputCol=\"Component\", outputCol=\"Ing_Component\")\n",
    "# featurizedData = hashingTF.transform(componentData)\n",
    "# idf = IDF(inputCol=\"Ing_Component\", outputCol=\"Ing_Feature\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# normalizer = Normalizer(inputCol=\"Ing_Feature\", outputCol=\"norm\")\n",
    "# similarity = normalizer.transform(rescaledData)\n",
    "# sim_matrix = similarity.select(\"ID\", \"title\", \"norm\")\n",
    "# sim_matrix.show()\n",
    "# sim_test = sim_matrix.rdd.map(lambda row:IndexedRow(row.ID, row.norm.toArray()))\n",
    "# type(sim_test)\n",
    "\n",
    "# mat = IndexedRowMatrix(similarity.select(\"title\", \"norm\")\\\n",
    "#                      .rdd.map(lambda row: IndexedRow(row.title, row.norm.toArray()))).toBlockMatrix()\n",
    "# dot = mat.multiply(mat.transpose())\n",
    "# dot.toLocalMatrix().toArray()\n",
    "\n",
    "# dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "# data.alias(\"i\").join(data.alias(\"j\"), col(\"i.title\") < col(\"j.title\"))\\\n",
    "#     .select(\n",
    "#         col(\"i.title\").alias(\"i\"), \n",
    "#         col(\"j.title\").alias(\"j\"), \n",
    "#         dot_udf(\"i.norm\", \"j.norm\").alias(\"dot\"))\\\n",
    "#     .sort(\"i\", \"j\")\\\n",
    "#     .show()\n",
    "\n",
    "# sim_mat = IndexedRowMatrix(sim_test).toBlockMatrix()\n",
    "# dot = sim_mat.multiply(sim_mat.transpose())\n",
    "# dot.toLocalMatrix().toArray()\n",
    "# dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "\n",
    "# similarity.alias(\"i\").join(similarity.alias(\"j\"), col(\"i.ID\") < col(\"j.ID\"))\\\n",
    "#     .select(\n",
    "#         col(\"i.ID\").alias(\"i\"), \n",
    "#         col(\"j.ID\").alias(\"j\"), \n",
    "#         dot_udf(\"i.ID\", \"j.ID\").alias(\"cosine\"))\\\n",
    "#     .sort(\"i\", \"j\")\\\n",
    "#     .show()\n",
    "\n",
    "# cv = CountVectorizer(inputCol=\"Ing_Str\", outputCol=\"Ing_Feature\", vocabSize=3, minDF=2.0)\n",
    "# model = cv.fit(cleanIngredient_table)\n",
    "# result = model.transform(cleanIngredient_table)\n",
    "# result.show(5)\n",
    "# test = result.toPandas()\n",
    "# test.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
