{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import chdir\n",
    "from itertools import compress\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf, explode, col, lit, monotonically_increasing_id, unix_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Normalizer, CountVectorizer\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import findspark\n",
    "findspark.init()\n",
    "print(\"spark ran\")\n",
    "\n",
    "chdir(\"C:\\\\Users\\\\chest\\\\Desktop\\\\MTech\\\\Big Data\\\\Project\\\\Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings necessary for sandboxing otherwise Pyspark too slow on standalone\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "sc = SparkContext(\"local\", \"App Name\")\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.setConf(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "sqlContext.setConf(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"12000\")\n",
    "#improve toPandas() \n",
    "sqlContext.setConf(\"spark.sql.execution.arrow.enabled\", \"true\"\")\n",
    "print(\"Spark Context Created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark SQL basic example\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "# print(\"Spark Session Created\")\n",
    "#Python Java issues: a drawback in using pyspark instead of Scala"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of filtering words\n",
    "with open(\"Overall_Master.txt\") as file:\n",
    "    ingredient_set = file.read().splitlines()\n",
    "print(\"List length: \" + str(len(ingredient_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\chest\\\\Desktop\\\\MTech\\\\Big Data\\\\Project\\\\Final\\\\bbccouk-recipes.json\"\n",
    "allbbc_df = sqlContext.read.json(path)\n",
    "allbbc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allbbc_df.createOrReplaceTempView(\"bbc_recipes\")\n",
    "\n",
    "#overall table\n",
    "allbbc_table = sqlContext.sql(\"SELECT title, ingredients, description, instructions, photo_url, url, total_time_minutes  FROM bbc_recipes ORDER BY RAND() LIMIT 1500\")\n",
    "allbbc_table.show(5)\n",
    "\n",
    "# allbbc_pandas = allbbc_table.toPandas()\n",
    "# allbbc_pandas.to_csv(\"allbbc_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User-defined Functions\n",
    "def removeParenthesis(li):\n",
    "    output = []\n",
    "    for text in li:\n",
    "        text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "        text = text.strip()\n",
    "        output.append(text)\n",
    "    return output\n",
    "\n",
    "def extractIngredient(li, ingredient_set):\n",
    "    output = []\n",
    "    for item in li:\n",
    "        temp_output = []\n",
    "        for set_tracker in range(len(ingredient_set)):\n",
    "            check = bool(re.search(ingredient_set[set_tracker], item))\n",
    "            if (check == True):    \n",
    "                temp_output.append(ingredient_set[set_tracker])\n",
    "        if (len(temp_output) != 0):\n",
    "            temp_counter = 0\n",
    "            temp_tracker = 0\n",
    "            for temp in range(len(temp_output)):\n",
    "                count_temp = len(temp_output[temp])\n",
    "                if (count_temp > temp_counter):\n",
    "                    temp_tracker = temp\n",
    "                    temp_counter = count_temp\n",
    "            output.append(temp_output[temp_tracker])\n",
    "#             output.append(temp_output[[len(i) for i in temp_output].index(max([len(i) for i in temp_output]))])\n",
    "    return output\n",
    "\n",
    "sample = ['1/2 cup unsalted butter, chilled and cubed', '1 cup chopped onion', '1 3/4 cups cornmeal', '1 1/4 cups all-purpose flour', '1/4 cup white sugar', '1 tablespoon baking powder', '1 1/2 teaspoons salt', '1/2 teaspoon baking soda', '1 1/2 cups buttermilk', '3 eggs', '1 1/2 cups shredded pepperjack cheese', '1 1/3 cups frozen corn kernels, thawed and drained', '2 ounces roasted marinated red bell peppers, drained and chopped', '1/2 cup chopped fresh basil']\n",
    "\n",
    "test = extractIngredient(sample, ingredient_set)\n",
    "print(test)\n",
    "print(len(ingredient_set))\n",
    "\n",
    "#create user defined function\n",
    "udf_removeParen = udf(removeParenthesis, ArrayType(StringType()))\n",
    "udf_extractIngredient = udf(lambda x: extractIngredient(x,ingredient_set), ArrayType(StringType()))\n",
    "udf_countList = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "print(\"UDF Successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Defined Function - Feature Engineering \n",
    "#Labels: Vegetarian, Lactose, Nut, Seafood\n",
    "\n",
    "#Safer to start off as non-vegetarian\n",
    "def detectVege(li, vegDetect_list):\n",
    "    label = 0\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in vegDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 1\n",
    "    return label\n",
    "\n",
    "#Safer to start off as positive allergy\n",
    "def detectNut(li):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (\"nut\" in text):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "def detectDairy(li):\n",
    "    label = 1\n",
    "    dairy_list = [\"cheese\", \"milk\", \"yoghurt\", \"cream\"]\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if (text in dairy_list):\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "def detectSeafood(li, seaDetect_list):\n",
    "    label = 1\n",
    "    detect_list = []\n",
    "    for text in li:\n",
    "        if text in seaDetect_list:\n",
    "            detect_list.append(text)\n",
    "    if (len(detect_list) == 0):\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "\n",
    "with open(\"vegDetect.txt\") as veg_file:\n",
    "    vegDetect_list = veg_file.read().splitlines()\n",
    "\n",
    "with open(\"seafoodDetect.txt\") as seafood_file:\n",
    "    seaDetect_list = seafood_file.read().splitlines()\n",
    "    \n",
    "#create user defined function\n",
    "udf_detectVege = udf(lambda x: detectVege(x, vegDetect_list), IntegerType())\n",
    "udf_detectNut = udf(detectNut, IntegerType())\n",
    "udf_detectDairy = udf(detectDairy, IntegerType())\n",
    "udf_detectSeafood = udf(lambda x: detectSeafood(x, seaDetect_list), IntegerType())\n",
    "\n",
    "#to stabilise schema with empty column\n",
    "# udf_empty = udf(lambda x: None, StringType())\n",
    "\n",
    "#create variable for time stamp\n",
    "timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ingredient table\n",
    "extracted_df = allbbc_table.withColumn('rm_paren', udf_removeParen('ingredients')) \\\n",
    "                           .withColumn('ingredient_extract', udf_extractIngredient('rm_paren'))\n",
    "\n",
    "extracted_df.cache()\n",
    "extracted_df.show(5)\n",
    "extracted_df.storageLevel\n",
    "# extracted_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this section creates recipe ingredient graph to store in HDFS\n",
    "ingredient_extract = extracted_df.select(\"title\", \"ingredient_extract\")\n",
    "\n",
    "ingredient_graph = ingredient_extract.withColumn('exploded',explode('ingredient_extract')) \\\n",
    "                                   .select(col('title').alias('Recipe'),col('exploded').alias('Ingredient'))\\\n",
    "\n",
    "ingredient_graph = ingredient_graph.filter(ingredient_graph.Ingredient != \"\")\n",
    "#                       .withColumn(\"Frequency\", lit(1))\n",
    "\n",
    "ingredient_graph.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create master dataframe\n",
    "labelled_df = extracted_df.withColumn(\"vegetarian_label\", udf_detectVege(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"nut_label\", udf_detectNut(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"lactose\", udf_detectDairy(\"ingredient_extract\")) \\\n",
    "                          .withColumn(\"seafood\", udf_detectSeafood(\"ingredient_extract\")) \\\n",
    "                          .withColumn('extract_count', udf_countList('ingredient_extract'))\\\n",
    "\n",
    "labelled_df = labelled_df.withColumn('timestamp', unix_timestamp(lit(timestamp),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\"))\n",
    "\n",
    "# labelled_df = labelled_df.withColumn(\"rating_stars\", udf_empty(\"ingredient_extract\")) \\\n",
    "#                          .withColumn(\"review_count\", udf_empty(\"ingredient_extract\"))\n",
    "\n",
    "\n",
    "labelled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['rm_paren', 'ingredient_extract']\n",
    "labelled_df = labelled_df.drop(*columns_to_drop)\n",
    "labelled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output\n",
    "#let's make our toPandas() faster\n",
    "labelled_pandas = labelled_df.toPandas()\n",
    "labelled_pandas.to_csv(\"bbc_label.csv\")\n",
    "# graph_pandas = ingredient_graph.toPandas()\n",
    "# graph_pandas.to_csv(\"ingredient_graph.csv\")\n",
    "\n",
    "#write json\n",
    "# labelled_json = labelled_df.toJSON()\n",
    "# json_output = labelled_json.collect()\n",
    "\n",
    "# with open(\"label_table.json\", \"w\") as file:\n",
    "#     for j in json_output:\n",
    "#         json.dump(j, file)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(json_output[2])\n",
    "with open(\"allbbc_label.json\", \"w\") as file:\n",
    "    json.dump(j, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create recipe-recipe graph\n",
    "recipeGraph_json = ingredient_extract.toJSON()\n",
    "recipeGraph_json = recipeGraph_json.collect()\n",
    "\n",
    "recipeGraph_list = []\n",
    "for js in recipeGraph_json:\n",
    "    js_output = json.loads(js)\n",
    "    recipeGraph_list.append(js_output)\n",
    "\n",
    "recipeA = []\n",
    "recipeB = []\n",
    "commonIng = []\n",
    "for first in range(len(recipeGraph_list)):\n",
    "    counter = 1\n",
    "    recipe_a = recipeGraph_list[first][\"title\"]\n",
    "    ing_a = recipeGraph_list[first][\"ingredient_extract\"]\n",
    "    for second in range(counter, len(recipeGraph_list)): \n",
    "        recipe_b = recipeGraph_list[second][\"title\"]\n",
    "        ing_b = recipeGraph_list[second][\"ingredient_extract\"]\n",
    "        common = len(set(ing_a).intersection(ing_b))\n",
    "        recipeA.append(recipe_a)\n",
    "        recipeB.append(recipe_b)\n",
    "        commonIng.append(common)\n",
    "    counter = counter + 1\n",
    "\n",
    "recipeGraph_dict = {\"recipeA\":recipeA, \"recipeB\":recipeB, \"common_ing\":commonIng}\n",
    "sharedRecipe_pandas = pd.DataFrame(recipeGraph_dict)\n",
    "sharedRecipe_pandas.to_csv(\"allbbc_sharedGraph.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Similarity matrix (Too expensive)\n",
    "# tokenizer = Tokenizer(inputCol=\"Ingredient\", outputCol=\"Component\")\n",
    "# componentData = tokenizer.transform(cleanIngredient_table)\n",
    "# hashingTF = HashingTF(inputCol=\"Component\", outputCol=\"Ing_Component\")\n",
    "# featurizedData = hashingTF.transform(componentData)\n",
    "# idf = IDF(inputCol=\"Ing_Component\", outputCol=\"Ing_Feature\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "# normalizer = Normalizer(inputCol=\"Ing_Feature\", outputCol=\"norm\")\n",
    "# similarity = normalizer.transform(rescaledData)\n",
    "# sim_matrix = similarity.select(\"ID\", \"title\", \"norm\")\n",
    "# sim_matrix.show()\n",
    "# sim_test = sim_matrix.rdd.map(lambda row:IndexedRow(row.ID, row.norm.toArray()))\n",
    "# type(sim_test)\n",
    "\n",
    "# mat = IndexedRowMatrix(similarity.select(\"title\", \"norm\")\\\n",
    "#                      .rdd.map(lambda row: IndexedRow(row.title, row.norm.toArray()))).toBlockMatrix()\n",
    "# dot = mat.multiply(mat.transpose())\n",
    "# dot.toLocalMatrix().toArray()\n",
    "\n",
    "# dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "# data.alias(\"i\").join(data.alias(\"j\"), col(\"i.title\") < col(\"j.title\"))\\\n",
    "#     .select(\n",
    "#         col(\"i.title\").alias(\"i\"), \n",
    "#         col(\"j.title\").alias(\"j\"), \n",
    "#         dot_udf(\"i.norm\", \"j.norm\").alias(\"dot\"))\\\n",
    "#     .sort(\"i\", \"j\")\\\n",
    "#     .show()\n",
    "\n",
    "# sim_mat = IndexedRowMatrix(sim_test).toBlockMatrix()\n",
    "# dot = sim_mat.multiply(sim_mat.transpose())\n",
    "# dot.toLocalMatrix().toArray()\n",
    "# dot_udf = udf(lambda x,y: float(x.dot(y)), DoubleType())\n",
    "\n",
    "# similarity.alias(\"i\").join(similarity.alias(\"j\"), col(\"i.ID\") < col(\"j.ID\"))\\\n",
    "#     .select(\n",
    "#         col(\"i.ID\").alias(\"i\"), \n",
    "#         col(\"j.ID\").alias(\"j\"), \n",
    "#         dot_udf(\"i.ID\", \"j.ID\").alias(\"cosine\"))\\\n",
    "#     .sort(\"i\", \"j\")\\\n",
    "#     .show()\n",
    "\n",
    "# cv = CountVectorizer(inputCol=\"Ing_Str\", outputCol=\"Ing_Feature\", vocabSize=3, minDF=2.0)\n",
    "# model = cv.fit(cleanIngredient_table)\n",
    "# result = model.transform(cleanIngredient_table)\n",
    "# result.show(5)\n",
    "# test = result.toPandas()\n",
    "# test.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
